{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees \n",
    "\n",
    "like in customer service\n",
    "    performing classification\n",
    "        class == tree of connection tissue\n",
    "\n",
    "\n",
    "## basically like a KD tree \n",
    "--> difference is that we assume that the K is pretty big \n",
    "\n",
    "often features are categorical\n",
    "\n",
    "tree often but not necessarily binary\n",
    "    internal nodes are deicision (question)\n",
    "    leaf nodes are predictions\n",
    "\n",
    "really easy to use this with categorical data, becuase just add nodes to make it so you can have multiple options of children\n",
    "\n",
    "leaf nodes: often all of the training points which fit in this bucket. often store the number --> kinda like voting \n",
    "    store # of training points in each class\n",
    "\n",
    "\n",
    "## BENEFITS:\n",
    "- unlinke KNN no exploitation - single path from root to leaf\n",
    "- potentially shorter trees \n",
    "- **understandability**  i see exactly the path of how they got there. we can look at this and see exactly how the classifier works --> understand how we got this  prediction\n",
    "    - MOST important feature at the root (like router plugged in being root)\n",
    "    - works well with CATEGORICAL and HETEROGENOUS features\n",
    "    - BIGGEST benefit of this \n",
    "\n",
    "\n",
    "HETEROGENOUS FEATURES EXAMPLE:\n",
    "ex: looking at caloirc intake vs SAT score\n",
    "    \n",
    "    KD and KNN would not do well because the data is like a strip, but we need to do circles in there\n",
    "    the space is really stretched\n",
    "    we could update KD tree to use distance differently --> but weird to work with\n",
    "    with a decision tree -- this doesn't matter if the units or ranges are vastly different. \n",
    "\n",
    "\n",
    "## DRAWBACKS:\n",
    "\n",
    "## How do we BUILD a Decision Tree?\n",
    "Ideally, leaf nodes are moslty a single class --> because that tells me i have found a region that is mostly one thing\n",
    "Ideally, internal nodes are \"important\" questions/features and they split the classes as well as possible \n",
    "\n",
    "**ID3 Algorithm:**\n",
    "greedy algorithm --> at each step we look at what is the best right now, not what is best in the future\n",
    "we have a set of training points AND a set of features\n",
    "```python\n",
    "if (the number of training points is small or the number of features is 0):\n",
    "     make leaf node\n",
    "else: //make internal node\n",
    "    for each feature:\n",
    "        find the best split value for that feature \n",
    "        make internal node using the best feature\n",
    "        remove feature from list\n",
    "        create a left and right child for it\n",
    "```\n",
    "\n",
    "**\"best split\"** --> evaluate split by pretending children will be leaves\n",
    "    being greedy at each layer and not looking forward\n",
    "\n",
    "**evaluating leaf nodes:** \n",
    "- SSR --> sum of squared residuals\n",
    "    - sum of squared(prediction-true) / (total number of points in the leaf) //fraction of wrong guesses\n",
    "    - want to decrease fraction of wrong guesses\n",
    "- GINI --> measure of inequality \n",
    "    -  0 if all points have same class\n",
    "-  ENTROPY -->\n",
    "    -  0 for all same class\n",
    "    \n",
    "\n",
    "## very easy to make DT with about 100% training accuracy. which is overfitting\n",
    "- very easy to overfit\n",
    "- makes a very tall tree\n",
    "\n",
    "how to fix \n",
    "- cap height\n",
    "- make a leaf size threshold (if there are fewer than 10 or 50 leaf points then i won't make a leaf node)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# BOOTSTRAPPING\n",
    "\n",
    "- data --> samples from some distribution\n",
    "- models --> are also samples from some distribution\n",
    "    - if we want to find the model from a model. like the average of the slope/mean from all of these models\n",
    "    - in reality, normally we are stuck with a single sample with N participants\n",
    "    - but we would like to model it to a larger distribution using only this one sample\n",
    "\n",
    "Bootstrapping --> turn one data set into MANY\n",
    "- pretend that our data set is like 100 data sets or something\n",
    "\n",
    "## How?\n",
    "- make a new data set\n",
    "- take random sample form original data set (can pick same entry over and over) with replacement\n",
    "- now we have a new dta set with the same size as the other one but it isn't identical\n",
    "\n",
    "## Bagging\n",
    "- reduces variance in decision trees \n",
    "----------- HOW??? --------------------------\n",
    "- bootstrap many data sets.\n",
    "- train an overfit data tree on each one\n",
    "- prediction is just a vote by all the decision trees\n",
    "- way of smoothing out or averaging out the noise. \n",
    "\n",
    "## Random Forests\n",
    "- build many decision trees from one data set\n",
    "- BUT we randomly limit which features we are allowed to split on at each node\n",
    "- then we vote to make a prediction\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
